{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6506b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c67657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16416aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_video\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import torchvision.models as torchmodels\n",
    "\n",
    "\n",
    "from fastai.distributed import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "from accelerate.utils import write_basic_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b575281",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1bca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(data_dir/'down.csv')\n",
    "# df.object_2 = 'None'\n",
    "# df.to_csv(data_dir/'down.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cca4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "180f7d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data_v4'); assert data_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0cd68e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 3, 512, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, _, _ = read_video(str(get_files(data_dir, extensions = ['.mp4'])[0]), output_format=\"TCHW\")\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df19b38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['into_hole',\n",
       "  'over',\n",
       "  'against',\n",
       "  'inside',\n",
       "  'outside',\n",
       "  'out',\n",
       "  'with',\n",
       "  'down',\n",
       "  'against_leaning',\n",
       "  'on',\n",
       "  'up',\n",
       "  'between',\n",
       "  'all_over',\n",
       "  'towards',\n",
       "  'above',\n",
       "  'among',\n",
       "  'behind',\n",
       "  'along',\n",
       "  'around_surround',\n",
       "  'below',\n",
       "  'by',\n",
       "  'into_crash',\n",
       "  'from',\n",
       "  'front',\n",
       "  'off',\n",
       "  'beside',\n",
       "  'around',\n",
       "  'through',\n",
       "  'onto',\n",
       "  'along_position'],\n",
       " 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [Path(cat).stem for cat in os.listdir(data_dir) if cat.endswith('.csv')]\n",
    "categories, len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e751a59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_categories = [ \n",
    "                    #'into_hole',\n",
    "#                       'over',\n",
    "#                       'against',\n",
    "                      'inside',\n",
    "                      'outside',\n",
    "#                       'out',\n",
    "#                       'with',\n",
    "#                       'down',\n",
    "                      'against_leaning',\n",
    "                      'on',\n",
    "#                       'up',\n",
    "                      'between',\n",
    "                      'all_over',\n",
    "#                       'towards',\n",
    "                      'above',\n",
    "                      'among',\n",
    "                      'behind',\n",
    "#                       'along',\n",
    "                      'around_surround',\n",
    "                      'below',\n",
    "#                       'by',\n",
    "#                       'into_crash',\n",
    "#                       'from',\n",
    "                      'front',\n",
    "#                       'off',\n",
    "                      'beside',\n",
    "#                       'around',\n",
    "#                       'through',\n",
    "#                       'onto',\n",
    "                      'along_position']\n",
    "\n",
    "dynamic_categories = [f for f in categories if f not in static_categories]\n",
    "\n",
    "len(static_categories), len(dynamic_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77c3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_and_object(obj_str:str):\n",
    "    color, object_name = ''.join(obj_str.split()[:-1]),obj_str.split()[-1]\n",
    "#     return self.colors_to_index[color], self.objects_to_index[object_name]\n",
    "    return color, object_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3ab0769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop(x): return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b826325",
   "metadata": {},
   "source": [
    "gonna have to create dataset in the fastai environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffad962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_oracle(Dataset):\n",
    "    def __init__(self, data_dir = data_dir, transform:list = None, y_transform:list = None, frames_per_vid=10, sampling = 'uniform'):\n",
    "        f'''\n",
    "        frames_per_vid: how many frames to consider per video for a data sample x. \n",
    "                        Per video, frames_per_vid images will be returned, using a sampling. Eg, if sampling = 'uniform', 10 uniformly spaced images will be returned. \n",
    "        sampling :      \"uniform\" by default, other values: \"start\", \"end\". if uniform, images will be uniformly returned (including the starting and ending images). If start, the first 10 images will be returned, \n",
    "                        if end, from the end. Alternatively, you can pass in a list of frames (eg [0,14,23,34, -1]) to choose from the video.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.data_dir = Path(data_dir)\n",
    "        \n",
    "        self.vocab = [Path(cat).stem for cat in os.listdir(data_dir) if cat.endswith('.csv')]\n",
    "        self.class_to_index = {cat:i for i,cat in enumerate(self.vocab)}\n",
    "        self.index_to_class = {self.class_to_index[cat]:cat for cat in self.vocab}\n",
    "        \n",
    "        self.c = len(self.vocab)\n",
    "        \n",
    "        self.frame_lim = frames_per_vid #only the last frames_per_vid \n",
    "        assert sampling in ('uniform', 'start', 'end') or type(sampling) == list, f\"invalid sampling technique. Choose from 'uniform', 'start' or 'end', or pass a list of frame numbers to choose. \"\n",
    "        self.sampling = sampling\n",
    "        \n",
    "        self.transforms = list(transform) if transform is not None else [self.sample, lambda x: x.float(), lambda x: torch.transpose(x,1,0)]\n",
    "        self.y_transforms = list(y_transform) if y_transform is not None else [lambda x: self.class_to_index[x]]\n",
    "        \n",
    "        self.videos = [f for f in get_files(data_dir, extensions=['.mp4']) if f.parent.stem in self.vocab] #(same as self.x)\n",
    "        self.ys = [vid.parent.stem for vid in self.videos]\n",
    "        \n",
    "        self.colors, self.objects = self._get_color_and_object_list(self.data_dir)\n",
    "    \n",
    "        self.objects_to_index = {cat:i for i,cat in enumerate(self.objects)}\n",
    "        self.colors_to_index = {cat:i for i,cat in enumerate(self.colors)}\n",
    "    \n",
    "    def __len__(self): return len(self.videos)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, _, _ = read_video(str(self.videos[idx]), output_format=\"TCHW\")\n",
    "        x=self.apply_tfm(x,self.transforms)\n",
    "        \n",
    "        y=self.apply_tfm(self.videos[idx].parent.stem, self.y_transforms)\n",
    "    \n",
    "        df_info = pd.read_csv(data_dir/f'{self.videos[idx].parent.stem}.csv')\n",
    "        obj1_val, obj2_val = df_info.loc[df_info['video_file_name']==str(self.videos[idx].name)][['object_1', 'object_2']].values[0]\n",
    "        \n",
    "        color1, obj1 = get_color_and_object(obj1_val)\n",
    "        color2, obj2 = get_color_and_object(obj2_val)\n",
    "        \n",
    "        assert color1 in self.colors and color2 in self.colors, f'{color1} or {color2} is an invalid color'\n",
    "        assert obj1 in self.objects and obj2 in self.objects, f'{obj1} or {obj2} is an invalid object name'\n",
    "           \n",
    "        return (x,(self.colors_to_index[color1], self.objects_to_index[obj1], self.colors_to_index[color2], self.objects_to_index[obj2])),y\n",
    "                         \n",
    "                         \n",
    "    def apply_tfm(self, x, tfm_list):\n",
    "        for tfm in tfm_list: x = tfm(x)\n",
    "        return x\n",
    "                       \n",
    "    def sample(self, frames):\n",
    "        if self.sampling=='uniform':  return frames[[int(i*(len(frames)-1)/(self.frame_lim-1)) for i in range(self.frame_lim)]]\n",
    "        elif self.sampling=='start': return frames[:self.frame_lim]\n",
    "        elif self.sampling=='end': return frames[len(frames)-self.frame_lim:]\n",
    "        elif type(self.sampling)==list: return frames[self.sampling]\n",
    "\n",
    "    #helper functions for getting attributes from csv files\n",
    "    \n",
    "    def _is_container(self,obj):\n",
    "        assert obj in self.objects\n",
    "        return obj.contains('_')\n",
    "    \n",
    "    def _get_color_and_object_list(self, data_dir, obj1_name = 'object_1', obj2_name='object_2'):\n",
    "        colors = set()\n",
    "        object_names = set()\n",
    "        for f in get_files(data_dir, extensions = ['.csv']):\n",
    "            df = pd.read_csv(f)\n",
    "            obj1_list = df[obj1_name].tolist()\n",
    "            obj2_list = df[obj2_name].tolist()\n",
    "            for obj_list in [obj1_list, obj2_list]:\n",
    "                for v in obj_list:\n",
    "                    c1, o1 = get_color_and_object(v)\n",
    "                    if c1 not in colors: colors.add(c1) \n",
    "                    if o1 not in object_names: object_names.add(o1)\n",
    "        \n",
    "        return list(colors), list(object_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e3482b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset_oracle(data_dir, sampling = 'uniform'); len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9073056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.colors), len(data.objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09458f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10, 512, 512]), 11, (4, 4, 3, 11))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x, objs_info),y = next(iter(data))\n",
    "x.shape, y, objs_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db859324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 6000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split = 0.8\n",
    "train_pct_sampling = 0.1\n",
    "\n",
    "train_ds, test_ds = torch.utils.data.random_split(data, [int(len(data)*train_split), len(data) - int(len(data)*train_split)])\n",
    "train_ds, _ = torch.utils.data.random_split(train_ds, [int(len(train_ds)*train_pct_sampling), len(train_ds) - int(len(train_ds)*train_pct_sampling)])\n",
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a753a69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2787, 3213)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_test_ds = torch.utils.data.Subset(data, indices = [idx for idx in test_ds.indices if data.ys[idx] in static_categories])\n",
    "dynamic_test_ds = torch.utils.data.Subset(data, indices = [idx for idx in test_ds.indices if data.ys[idx] in dynamic_categories])\n",
    "len(static_test_ds) , len(dynamic_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ce5a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bed1df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_size(frames_per_vid, max_effective_batch_size = 100):\n",
    "    bs_ideal = max_effective_batch_size//frames_per_vid\n",
    "    #this value can be a batch_size, but we want the nearest lower power of 2 as the batch size for efficiency purposes\n",
    "    pow=1\n",
    "    while True:\n",
    "        if pow*2<=bs_ideal: pow*=2\n",
    "        else: return pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73a2dbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch_size\n",
    "bs = get_batch_size(data.frame_lim, max_effective_batch_size = 80)\n",
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b91a6b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size =bs , shuffle = True)\n",
    "test_dl = DataLoader(test_ds, batch_size = 2*bs, shuffle = False)\n",
    "static_test_dl = DataLoader(static_test_ds, batch_size = 2*bs, shuffle = False)\n",
    "dynamic_test_dl = DataLoader(dynamic_test_ds, batch_size = 2*bs, shuffle = False)\n",
    "dls = DataLoaders(train_dl, test_dl,static_test_dl,dynamic_test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40d050d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.79 s, sys: 457 ms, total: 2.25 s\n",
      "Wall time: 487 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 3, 10, 512, 512]),\n",
       " tensor([ 3, 26,  4, 17, 15, 17, 11,  4]),\n",
       " [tensor([7, 1, 3, 1, 3, 1, 6, 8]),\n",
       "  tensor([ 4, 14, 14, 11, 14, 13, 12,  5]),\n",
       "  tensor([2, 4, 5, 4, 4, 7, 3, 4]),\n",
       "  tensor([ 7,  4, 10,  0, 13,  0, 14,  7])])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time (x, objs_info),y = next(iter(train_dl))\n",
    "x.shape, y, objs_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca211a1",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e9fe7",
   "metadata": {},
   "source": [
    "What if we used a pretrained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9a05be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.video.resnet import VideoResNet\n",
    "class custom_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, data, include_objects = False, include_colors = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stem = model.stem\n",
    "        self.layer1 = model.layer1\n",
    "        self.layer2 = model.layer2\n",
    "        self.layer3 = model.layer3\n",
    "        self.layer4 = model.layer4\n",
    "\n",
    "        self.avgpool = model.avgpool\n",
    "#         self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        self.include_colors = include_colors\n",
    "        self.include_objects = include_objects\n",
    "        \n",
    "        if self.include_objects:\n",
    "            self.objs = len(data.objects)\n",
    "            self.obj_embedding =   nn.Embedding(len(data.objects), 128)\n",
    "            \n",
    "        if self.include_colors:\n",
    "            self.colors = len(data.colors)\n",
    "            self.color_embedding = nn.Embedding(len(data.colors), 128)\n",
    "\n",
    "        self.fc = nn.Linear(in_features = 512 + 256*int(self.include_objects) + 256*int(self.include_colors)  , out_features=data.c)\n",
    "#         \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x,obj_info = x\n",
    "        c1,o1,c2,o2 = obj_info\n",
    "        \n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.flatten(1)\n",
    "\n",
    "        if self.include_objects:  \n",
    "            o1_emb, o2_emb = self.obj_embedding(o1), self.obj_embedding(o2)            \n",
    "            x = torch.cat((x,o1_emb, o2_emb), dim=1)\n",
    "            \n",
    "        if self.include_colors: \n",
    "            c1_emb, c2_emb = self.color_embedding(c1), self.color_embedding(c2)\n",
    "            x = torch.cat((x,c1_emb, c2_emb), dim=1)\n",
    "            \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "# .resnet import VideoResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2296aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "custom_model(\n",
       "  (stem): BasicStem(\n",
       "    (0): Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3d_18 = torchmodels.video.r3d_18(weights = 'KINETICS400_V1')\n",
    "# r3d_18 = torchmodels.video.r3d_18(weights = None)\n",
    "model = custom_model(r3d_18,data)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05c6d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(train_dl, test_dl,static_test_dl,dynamic_test_dl)\n",
    "learn = Learner(dls, model = model.to(device), loss_func = CrossEntropyLossFlat(), metrics = accuracy)\n",
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbc5f778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.407236</td>\n",
       "      <td>1.065155</td>\n",
       "      <td>0.784500</td>\n",
       "      <td>23:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"1f055f97-5307-4319-985d-60aa01bb9023\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"1f055f97-5307-4319-985d-60aa01bb9023\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "learn.fit_one_cycle(1, 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a09dceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/7.3.1.pth')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('7.3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fca68b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x7f080c26fca0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('7.3.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a5b904",
   "metadata": {},
   "source": [
    "## Checking accuracy on static variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9aaed39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "static_val = learn.validate(ds_idx =2)\n",
    "dynamic_val = learn.validate(ds_idx = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c14b5bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6239684224128723, 0.9237473011016846)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_val[1], dynamic_val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7df19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dl.vocab = [data.vocab] #theres a bug in fastai, it \n",
    "# interpret = ClassificationInterpretation.from_learner(learn, dl =test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "554e16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1e11ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1deb48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret.most_confused()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99ee6c",
   "metadata": {},
   "source": [
    "# Modified R3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83383946",
   "metadata": {},
   "source": [
    "## Training iterations\n",
    "\n",
    "1. only objects, no color. Embedding = 256+256\n",
    "2. object + color. Embedding = 128 + 128 + 128 +128\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39bbd22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5e210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4014af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab4b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6c844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184554a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
