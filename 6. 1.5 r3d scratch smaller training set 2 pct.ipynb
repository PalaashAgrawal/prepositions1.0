{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6506b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c67657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16416aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_video\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import torchvision.models as torchmodels\n",
    "\n",
    "\n",
    "from fastai.distributed import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "from accelerate.utils import write_basic_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b575281",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1bca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(data_dir/'down.csv')\n",
    "# df.object_2 = 'None'\n",
    "# df.to_csv(data_dir/'down.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cca4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "180f7d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data_v4'); assert data_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0cd68e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 3, 512, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, _, _ = read_video(str(get_files(data_dir, extensions = ['.mp4'])[0]), output_format=\"TCHW\")\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df19b38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['into_hole',\n",
       "  'over',\n",
       "  'against',\n",
       "  'inside',\n",
       "  'outside',\n",
       "  'out',\n",
       "  'with',\n",
       "  'down',\n",
       "  'against_leaning',\n",
       "  'on',\n",
       "  'up',\n",
       "  'between',\n",
       "  'all_over',\n",
       "  'towards',\n",
       "  'above',\n",
       "  'among',\n",
       "  'behind',\n",
       "  'along',\n",
       "  'around_surround',\n",
       "  'below',\n",
       "  'by',\n",
       "  'into_crash',\n",
       "  'from',\n",
       "  'front',\n",
       "  'off',\n",
       "  'beside',\n",
       "  'around',\n",
       "  'through',\n",
       "  'onto',\n",
       "  'along_position'],\n",
       " 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [Path(cat).stem for cat in os.listdir(data_dir) if cat.endswith('.csv')]\n",
    "categories, len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a77c3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_and_object(obj_str:str):\n",
    "    color, object_name = ''.join(obj_str.split()[:-1]),obj_str.split()[-1]\n",
    "#     return self.colors_to_index[color], self.objects_to_index[object_name]\n",
    "    return color, object_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ab0769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop(x): return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b826325",
   "metadata": {},
   "source": [
    "gonna have to create dataset in the fastai environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffad962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_oracle(Dataset):\n",
    "    def __init__(self, data_dir = data_dir, transform:list = None, y_transform:list = None, frames_per_vid=10, sampling = 'uniform'):\n",
    "        f'''\n",
    "        frames_per_vid: how many frames to consider per video for a data sample x. \n",
    "                        Per video, frames_per_vid images will be returned, using a sampling. Eg, if sampling = 'uniform', 10 uniformly spaced images will be returned. \n",
    "        sampling :      \"uniform\" by default, other values: \"start\", \"end\". if uniform, images will be uniformly returned (including the starting and ending images). If start, the first 10 images will be returned, \n",
    "                        if end, from the end. Alternatively, you can pass in a list of frames (eg [0,14,23,34, -1]) to choose from the video.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.data_dir = Path(data_dir)\n",
    "        \n",
    "        self.vocab = [Path(cat).stem for cat in os.listdir(data_dir) if cat.endswith('.csv')]\n",
    "        self.class_to_index = {cat:i for i,cat in enumerate(self.vocab)}\n",
    "        self.index_to_class = {self.class_to_index[cat]:cat for cat in self.vocab}\n",
    "        \n",
    "        self.c = len(self.vocab)\n",
    "        \n",
    "        self.frame_lim = frames_per_vid #only the last frames_per_vid \n",
    "        assert sampling in ('uniform', 'start', 'end') or type(sampling) == list, f\"invalid sampling technique. Choose from 'uniform', 'start' or 'end', or pass a list of frame numbers to choose. \"\n",
    "        self.sampling = sampling\n",
    "        \n",
    "        self.transforms = list(transform) if transform is not None else [self.sample, lambda x: x.float(), lambda x: torch.transpose(x,1,0)]\n",
    "        self.y_transforms = list(y_transform) if y_transform is not None else [lambda x: self.class_to_index[x]]\n",
    "        \n",
    "        self.videos = [f for f in get_files(data_dir, extensions=['.mp4']) if f.parent.stem in self.vocab]\n",
    "        \n",
    "        \n",
    "        self.colors, self.objects = self._get_color_and_object_list(self.data_dir)\n",
    "    \n",
    "        self.objects_to_index = {cat:i for i,cat in enumerate(self.objects)}\n",
    "        self.colors_to_index = {cat:i for i,cat in enumerate(self.colors)}\n",
    "    \n",
    "    def __len__(self): return len(self.videos)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, _, _ = read_video(str(self.videos[idx]), output_format=\"TCHW\")\n",
    "        x=self.apply_tfm(x,self.transforms)\n",
    "        \n",
    "        y=self.apply_tfm(self.videos[idx].parent.stem, self.y_transforms)\n",
    "    \n",
    "        df_info = pd.read_csv(data_dir/f'{self.videos[idx].parent.stem}.csv')\n",
    "        obj1_val, obj2_val = df_info.loc[df_info['video_file_name']==str(self.videos[idx].name)][['object_1', 'object_2']].values[0]\n",
    "        \n",
    "        color1, obj1 = get_color_and_object(obj1_val)\n",
    "        color2, obj2 = get_color_and_object(obj2_val)\n",
    "        \n",
    "        assert color1 in self.colors and color2 in self.colors, f'{color1} or {color2} is an invalid color'\n",
    "        assert obj1 in self.objects and obj2 in self.objects, f'{obj1} or {obj2} is an invalid object name'\n",
    "           \n",
    "        return (x,(self.colors_to_index[color1], self.objects_to_index[obj1], self.colors_to_index[color2], self.objects_to_index[obj2])),y\n",
    "                         \n",
    "                         \n",
    "    def apply_tfm(self, x, tfm_list):\n",
    "        for tfm in tfm_list: x = tfm(x)\n",
    "        return x\n",
    "                       \n",
    "    def sample(self, frames):\n",
    "        if self.sampling=='uniform':  return frames[[int(i*(len(frames)-1)/(self.frame_lim-1)) for i in range(self.frame_lim)]]\n",
    "        elif self.sampling=='start': return frames[:self.frame_lim]\n",
    "        elif self.sampling=='end': return frames[len(frames)-self.frame_lim:]\n",
    "        elif type(self.sampling)==list: return frames[self.sampling]\n",
    "\n",
    "    #helper functions for getting attributes from csv files\n",
    "    \n",
    "    def _is_container(self,obj):\n",
    "        assert obj in self.objects\n",
    "        return obj.contains('_')\n",
    "    \n",
    "    def _get_color_and_object_list(self, data_dir, obj1_name = 'object_1', obj2_name='object_2'):\n",
    "        colors = set()\n",
    "        object_names = set()\n",
    "        for f in get_files(data_dir, extensions = ['.csv']):\n",
    "            df = pd.read_csv(f)\n",
    "            obj1_list = df[obj1_name].tolist()\n",
    "            obj2_list = df[obj2_name].tolist()\n",
    "            for obj_list in [obj1_list, obj2_list]:\n",
    "                for v in obj_list:\n",
    "                    c1, o1 = get_color_and_object(v)\n",
    "                    if c1 not in colors: colors.add(c1) \n",
    "                    if o1 not in object_names: object_names.add(o1)\n",
    "        \n",
    "        return list(colors), list(object_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e3482b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset_oracle(data_dir, sampling = 'uniform'); len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9073056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 15)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.colors), len(data.objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09458f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10, 512, 512]), 11, (2, 5, 3, 3))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x, objs_info),y = next(iter(data))\n",
    "x.shape, y, objs_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db859324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 6000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split = 0.8\n",
    "train_pct_sampling = 0.02\n",
    "\n",
    "train_ds, test_ds = torch.utils.data.random_split(data, [int(len(data)*train_split), len(data) - int(len(data)*train_split)])\n",
    "train_ds, _ = torch.utils.data.random_split(train_ds, [int(len(train_ds)*train_pct_sampling), len(train_ds) - int(len(train_ds)*train_pct_sampling)])\n",
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ce5a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bed1df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_size(frames_per_vid, max_effective_batch_size = 100):\n",
    "    bs_ideal = max_effective_batch_size//frames_per_vid\n",
    "    #this value can be a batch_size, but we want the nearest lower power of 2 as the batch size for efficiency purposes\n",
    "    pow=1\n",
    "    while True:\n",
    "        if pow*2<=bs_ideal: pow*=2\n",
    "        else: return pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73a2dbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch_size\n",
    "bs = get_batch_size(data.frame_lim, max_effective_batch_size = 80)\n",
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b91a6b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size =bs , shuffle = True)\n",
    "test_dl = DataLoader(test_ds, batch_size = 2*bs, shuffle = True)\n",
    "dls = DataLoaders(train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40d050d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.9 s, sys: 203 ms, total: 2.1 s\n",
      "Wall time: 489 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 3, 10, 512, 512]),\n",
       " tensor([ 3,  4,  1,  2,  5, 29, 16, 13]),\n",
       " [tensor([5, 3, 5, 7, 6, 1, 4, 6]),\n",
       "  tensor([ 5,  9, 13,  9, 10,  3,  2, 14]),\n",
       "  tensor([7, 1, 7, 1, 8, 5, 2, 5]),\n",
       "  tensor([12, 11,  2, 13, 11,  1,  7, 14])])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time (x, objs_info),y = next(iter(train_dl))\n",
    "x.shape, y, objs_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca211a1",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e9fe7",
   "metadata": {},
   "source": [
    "What if we used a pretrained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9a05be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.video.resnet import VideoResNet\n",
    "class custom_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, data, include_objects = False, include_colors = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stem = model.stem\n",
    "        self.layer1 = model.layer1\n",
    "        self.layer2 = model.layer2\n",
    "        self.layer3 = model.layer3\n",
    "        self.layer4 = model.layer4\n",
    "\n",
    "        self.avgpool = model.avgpool\n",
    "#         self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        self.include_colors = include_colors\n",
    "        self.include_objects = include_objects\n",
    "        \n",
    "        if self.include_objects:\n",
    "            self.objs = len(data.objects)\n",
    "            self.obj_embedding =   nn.Embedding(len(data.objects), 128)\n",
    "            \n",
    "        if self.include_colors:\n",
    "            self.colors = len(data.colors)\n",
    "            self.color_embedding = nn.Embedding(len(data.colors), 128)\n",
    "\n",
    "        self.fc = nn.Linear(in_features = 512 + 256*int(self.include_objects) + 256*int(self.include_colors)  , out_features=data.c)\n",
    "#         \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x,obj_info = x\n",
    "        c1,o1,c2,o2 = obj_info\n",
    "        \n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.flatten(1)\n",
    "\n",
    "        if self.include_objects:  \n",
    "            o1_emb, o2_emb = self.obj_embedding(o1), self.obj_embedding(o2)            \n",
    "            x = torch.cat((x,o1_emb, o2_emb), dim=1)\n",
    "            \n",
    "        if self.include_colors: \n",
    "            c1_emb, c2_emb = self.color_embedding(c1), self.color_embedding(c2)\n",
    "            x = torch.cat((x,c1_emb, c2_emb), dim=1)\n",
    "            \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "# .resnet import VideoResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2296aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "custom_model(\n",
       "  (stem): BasicStem(\n",
       "    (0): Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r3d_18 = torchmodels.video.r3d_18(weights = 'KINETICS400_V1')\n",
    "r3d_18 = torchmodels.video.r3d_18(weights = None)\n",
    "model = custom_model(r3d_18,data)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05c6d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(train_dl, test_dl)\n",
    "learn = Learner(dls, model = model.to(device), loss_func = CrossEntropyLossFlat(), metrics = accuracy)\n",
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbc5f778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.290753</td>\n",
       "      <td>3.063956</td>\n",
       "      <td>0.117667</td>\n",
       "      <td>13:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"d97c234f-2e35-44e9-8039-35dc1dd42f30\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"d97c234f-2e35-44e9-8039-35dc1dd42f30\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "learn.fit_one_cycle(1, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7df19e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='6' class='' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.60% [6/375 00:10&lt;10:34 3.2908]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_dl\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m [data\u001b[38;5;241m.\u001b[39mvocab] \u001b[38;5;66;03m#theres a bug in fastai, it \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m interpret \u001b[38;5;241m=\u001b[39m \u001b[43mClassificationInterpretation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_learner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/prepositions/lib/python3.9/site-packages/fastai/interpret.py:52\u001b[0m, in \u001b[0;36mInterpretation.from_learner\u001b[0;34m(cls, learn, ds_idx, dl, act)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConstruct interpretation object from a learner\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dl \u001b[38;5;241m=\u001b[39m learn\u001b[38;5;241m.\u001b[39mdls[ds_idx]\u001b[38;5;241m.\u001b[39mnew(shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 52\u001b[0m _,_,losses \u001b[38;5;241m=\u001b[39m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_decoded\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mwith_preds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_targs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(learn, dl, losses, act)\n",
      "File \u001b[0;32m~/miniconda3/envs/prepositions/lib/python3.9/site-packages/fastai/learner.py:300\u001b[0m, in \u001b[0;36mLearner.get_preds\u001b[0;34m(self, ds_idx, dl, with_input, with_decoded, with_loss, act, inner, reorder, cbs, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_loss: ctx_mgrs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_not_reduced())\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(ctx_mgrs):\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m act \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: act \u001b[38;5;241m=\u001b[39m getcallable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    302\u001b[0m     res \u001b[38;5;241m=\u001b[39m cb\u001b[38;5;241m.\u001b[39mall_tensors()\n",
      "File \u001b[0;32m~/miniconda3/envs/prepositions/lib/python3.9/site-packages/fastai/learner.py:236\u001b[0m, in \u001b[0;36mLearner._do_epoch_validate\u001b[0;34m(self, ds_idx, dl)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls[ds_idx]\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m dl\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelValidException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/prepositions/lib/python3.9/site-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/miniconda3/envs/prepositions/lib/python3.9/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_batch(\u001b[38;5;241m*\u001b[39mo)\n",
      "File \u001b[0;32m~/miniconda3/envs/prepositions/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/prepositions/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/prepositions/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/prepositions/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/prepositions/lib/python3.9/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn [12], line 36\u001b[0m, in \u001b[0;36mdataset_oracle.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 36\u001b[0m     x, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mread_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideos\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTCHW\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_tfm(x,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms)\n\u001b[1;32m     39\u001b[0m     y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_tfm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideos[idx]\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mstem, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_transforms)\n",
      "File \u001b[0;32m~/miniconda3/envs/prepositions/lib/python3.9/site-packages/torchvision/io/video.py:298\u001b[0m, in \u001b[0;36mread_video\u001b[0;34m(filename, start_pts, end_pts, pts_unit, output_format)\u001b[0m\n\u001b[1;32m    296\u001b[0m     audio_timebase \u001b[38;5;241m=\u001b[39m container\u001b[38;5;241m.\u001b[39mstreams\u001b[38;5;241m.\u001b[39maudio[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtime_base\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m container\u001b[38;5;241m.\u001b[39mstreams\u001b[38;5;241m.\u001b[39mvideo:\n\u001b[0;32m--> 298\u001b[0m     video_frames \u001b[38;5;241m=\u001b[39m \u001b[43m_read_from_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_pts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_pts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpts_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     video_fps \u001b[38;5;241m=\u001b[39m container\u001b[38;5;241m.\u001b[39mstreams\u001b[38;5;241m.\u001b[39mvideo[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39maverage_rate\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;66;03m# guard against potentially corrupted files\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/prepositions/lib/python3.9/site-packages/torchvision/io/video.py:153\u001b[0m, in \u001b[0;36m_read_from_stream\u001b[0;34m(container, start_offset, end_offset, pts_unit, stream, stream_name)\u001b[0m\n\u001b[1;32m    151\u001b[0m _CALLED_TIMES \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _CALLED_TIMES \u001b[38;5;241m%\u001b[39m _GC_COLLECTION_INTERVAL \u001b[38;5;241m==\u001b[39m _GC_COLLECTION_INTERVAL \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[43mgc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pts_unit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msec\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# TODO: we should change all of this from ground up to simply take\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# sec and convert to MS in C++\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     start_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mfloor(start_offset \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m stream\u001b[38;5;241m.\u001b[39mtime_base)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test_dl.vocab = [data.vocab] #theres a bug in fastai, it \n",
    "# interpret = ClassificationInterpretation.from_learner(learn, dl =test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e11ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1deb48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret.most_confused()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99ee6c",
   "metadata": {},
   "source": [
    "# Modified R3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83383946",
   "metadata": {},
   "source": [
    "## Training iterations\n",
    "\n",
    "1. only objects, no color. Embedding = 256+256\n",
    "2. object + color. Embedding = 128 + 128 + 128 +128\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39bbd22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5e210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4014af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab4b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6c844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184554a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
